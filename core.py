"""
Core logic for KF Searcher
Based on VS5 core module, adapted for Kufar.by
"""

import logging
import time
import random
import pytz
from datetime import datetime
from typing import List, Dict, Optional, Any
from urllib.parse import urlparse, parse_qs

from pyKufarVN import Kufar, KufarException, KufarAPIException
from db import get_db
from configuration_values import (
    get_max_items_per_search,
    ERROR_CODES_FOR_REDEPLOY,
    KF_REGIONS,
    KF_CATEGORIES
)

logger = logging.getLogger(__name__)

class KufarSearcher:
    """Main searcher class for Kufar.by monitoring"""
    
    def __init__(self):
        self.kufar_client = None
        self.error_count = 0
        self.last_proxy_change = 0
        
        # Initialize Kufar client
        self._init_kufar_client()
    
    def _init_kufar_client(self):
        """Initialize Kufar API client"""
        try:
            self.kufar_client = Kufar()
            
            # Test connection
            if not self.kufar_client.test_connection():
                logger.warning("Initial connection test failed")
                self._change_proxy_if_needed()
            else:
                logger.info("Kufar client initialized successfully")
            get_db().add_log_entry('INFO', 'Kufar client initialized successfully', 'KufarSearcher')
                
        except Exception as e:
            logger.error(f"Failed to initialize Kufar client: {e}")
            raise
    
    def _change_proxy_if_needed(self):
        """Change proxy if connection issues"""
        current_time = time.time()
        
        # Change proxy max once per 5 minutes
        if current_time - self.last_proxy_change > 300:
            try:
                self.kufar_client.change_proxy()
                self.last_proxy_change = current_time
                logger.info("Changed proxy due to connection issues")
            except Exception as e:
                logger.error(f"Failed to change proxy: {e}")
    
    def search_all_queries(self) -> Dict[str, Any]:
        """Search all active queries and return results summary - Ð¿Ñ€Ð¾ÑÑ‚Ð°Ñ Ð»Ð¾Ð³Ð¸ÐºÐ°"""
        logger.info("ðŸ” Ð—Ð°Ð¿ÑƒÑÐº Ñ†Ð¸ÐºÐ»Ð° ÑÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ")
        get_db().add_log_entry('INFO', 'Ð—Ð°Ð¿ÑƒÑÐº Ñ†Ð¸ÐºÐ»Ð° ÑÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ', 'core', 'ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð³Ð¾Ñ‚Ð¾Ð²Ð½Ð¾ÑÑ‚Ð¸ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð² Ðº Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸ÑŽ')
        
        results = {
            'total_searches': 0,
            'ready_for_scan': 0,
            'successful_searches': 0,
            'failed_searches': 0,
            'new_items': 0,
            'skipped_searches': 0,
            'errors': []
        }
        
        try:
            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÑƒ Ð¸Ð½Ñ‚ÐµÑ€Ð²Ð°Ð»Ð° ÑÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
            from configuration_values import get_search_interval
            interval_seconds = get_search_interval()
            
            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð’Ð¡Ð• Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð¿Ð¾Ð¸ÑÐºÐ¸ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
            all_searches = get_db().get_active_searches()
            results['total_searches'] = len(all_searches)
            
            if not all_searches:
                logger.info("âŒ ÐÐµÑ‚ Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ñ… Ð¿Ð¾Ð¸ÑÐºÐ¾Ð² Ð² ÑÐ¸ÑÑ‚ÐµÐ¼Ðµ")
                get_db().add_log_entry('INFO', 'ÐÐµÑ‚ Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ñ… Ð¿Ð¾Ð¸ÑÐºÐ¾Ð² Ð² ÑÐ¸ÑÑ‚ÐµÐ¼Ðµ', 'core', 'Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð¿Ñ€Ð¾ÑÑ‚Ð°Ð¸Ð²Ð°ÐµÑ‚')
                return results
            
            # ÐŸÑ€Ð¾ÑÑ‚Ð°Ñ Ð»Ð¾Ð³Ð¸ÐºÐ°: Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ ÐºÐ°ÐºÐ¸Ðµ Ð¿Ð¾Ð¸ÑÐºÐ¸ Ð³Ð¾Ñ‚Ð¾Ð²Ñ‹ Ð´Ð»Ñ ÑÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
            ready_searches = []
            now = get_db().get_belarus_time()
            
            logger.info(f"ðŸ“‹ ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ {len(all_searches)} Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ñ… Ð¿Ð¾Ð¸ÑÐºÐ¾Ð² (Ð¸Ð½Ñ‚ÐµÑ€Ð²Ð°Ð»: {interval_seconds}Ñ)")
            
            for search in all_searches:
                last_scan = search.get('last_scan_time')
                if not last_scan:
                    # ÐÐ¸ÐºÐ¾Ð³Ð´Ð° Ð½Ðµ ÑÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð»ÑÑ - Ð³Ð¾Ñ‚Ð¾Ð² Ðº ÑÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸ÑŽ
                    ready_searches.append(search)
                    logger.info(f"âœ… '{search['name']}': Ð½Ð¸ÐºÐ¾Ð³Ð´Ð° Ð½Ðµ ÑÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð»ÑÑ, Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð² Ð¾Ñ‡ÐµÑ€ÐµÐ´ÑŒ")
                else:
                    # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ Ð²Ñ€ÐµÐ¼Ñ Ð² Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ñ‹Ð¹ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾
                    if isinstance(last_scan, str):
                        try:
                            # ÐŸÐ°Ñ€ÑÐ¸Ð¼ ÑÑ‚Ñ€Ð¾ÐºÑƒ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸
                            last_scan = datetime.fromisoformat(last_scan.replace('Z', '+00:00'))
                            # ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð² Ð±ÐµÐ»Ð¾Ñ€ÑƒÑÑÐºÐ¸Ð¹ Ñ‡Ð°ÑÐ¾Ð²Ð¾Ð¹ Ð¿Ð¾ÑÑ
                            if last_scan.tzinfo is None:
                                last_scan = last_scan.replace(tzinfo=pytz.UTC)
                            from db import BELARUS_TZ
                            last_scan = last_scan.astimezone(BELARUS_TZ)
                        except:
                            logger.warning(f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ñ€Ð°ÑÐ¿Ð°Ñ€ÑÐ¸Ñ‚ÑŒ Ð²Ñ€ÐµÐ¼Ñ Ð´Ð»Ñ '{search['name']}', ÑÑ‡Ð¸Ñ‚Ð°ÐµÐ¼ Ð³Ð¾Ñ‚Ð¾Ð²Ñ‹Ð¼")
                            ready_searches.append(search)
                            continue
                    elif hasattr(last_scan, 'tzinfo') and last_scan.tzinfo is None:
                        # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ñ‡Ð°ÑÐ¾Ð²Ð¾Ð¹ Ð¿Ð¾ÑÑ ÐµÑÐ»Ð¸ ÐµÐ³Ð¾ Ð½ÐµÑ‚
                        from db import BELARUS_TZ
                        last_scan = last_scan.replace(tzinfo=BELARUS_TZ)
                    
                    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð¿Ñ€Ð¾ÑˆÐµÐ» Ð»Ð¸ Ð¸Ð½Ñ‚ÐµÑ€Ð²Ð°Ð»
                    time_since_scan = (now - last_scan).total_seconds()
                    if time_since_scan >= interval_seconds:
                        ready_searches.append(search)
                        minutes_ago = int(time_since_scan // 60)
                        seconds_ago = int(time_since_scan % 60)
                        logger.info(f"âœ… '{search['name']}': Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐµ ÑÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ {minutes_ago}Ð¼ {seconds_ago}Ñ Ð½Ð°Ð·Ð°Ð´ (Ð¸Ð½Ñ‚ÐµÑ€Ð²Ð°Ð»: {interval_seconds}Ñ)")
                    else:
                        remaining = interval_seconds - time_since_scan
                        remaining_minutes = int(remaining // 60)
                        remaining_seconds = int(remaining % 60)
                        logger.debug(f"â±ï¸ '{search['name']}': Ð¾Ð¶Ð¸Ð´Ð°Ð½Ð¸Ðµ ÐµÑ‰Ðµ {remaining_minutes}Ð¼ {remaining_seconds}Ñ")
                        results['skipped_searches'] += 1
            
            results['ready_for_scan'] = len(ready_searches)
            
            if not ready_searches:
                logger.info(f"â±ï¸ ÐÐµÑ‚ Ð¿Ð¾Ð¸ÑÐºÐ¾Ð² Ð³Ð¾Ñ‚Ð¾Ð²Ñ‹Ñ… Ð´Ð»Ñ ÑÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ (Ð¸Ð½Ñ‚ÐµÑ€Ð²Ð°Ð»: {interval_seconds}Ñ)")
                get_db().add_log_entry('INFO', f'Ð’ÑÐµ Ð¿Ð¾Ð¸ÑÐºÐ¸ Ð¾Ð¶Ð¸Ð´Ð°ÑŽÑ‚ Ð¸Ð½Ñ‚ÐµÑ€Ð²Ð°Ð»Ð° ({interval_seconds}Ñ)', 'core', 'Ð¡Ð»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ Ñ†Ð¸ÐºÐ» Ñ‡ÐµÑ€ÐµÐ· Ð¼Ð¸Ð½ÑƒÑ‚Ñƒ')
                return results
            
            logger.info(f"ðŸš€ ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ ÑÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ {len(ready_searches)} Ð³Ð¾Ñ‚Ð¾Ð²Ñ‹Ñ… Ð¿Ð¾Ð¸ÑÐºÐ¾Ð²")
            get_db().add_log_entry('INFO', f'Ð¡ÐºÐ°Ð½Ð¸Ñ€ÑƒÐµÐ¼ {len(ready_searches)} Ð³Ð¾Ñ‚Ð¾Ð²Ñ‹Ñ… Ð¿Ð¾Ð¸ÑÐºÐ¾Ð²', 'core', f'ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ {len(ready_searches)} Ð¸Ð· {len(all_searches)} Ð¿Ð¾Ð¸ÑÐºÐ¾Ð²')
            
            # ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ ÐºÐ°Ð¶Ð´Ñ‹Ð¹ Ð³Ð¾Ñ‚Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº
            for search in ready_searches:
                try:
                    logger.info(f"Processing search: {search['name']} (ID: {search['id']})")
                    get_db().add_log_entry('INFO', f"[DEBUG] Processing search: {search['name']}", 'core', f"Starting search for query ID {search['id']}")
                    
                    # Search for items
                    items = self.search_query(search)
                    total_items = len(items)
                    
                    if items:
                        # Process new items
                        new_items = self._process_new_items(items, search)
                        new_count = len(new_items)
                        duplicate_count = total_items - new_count
                        
                        results['new_items'] += new_count
                        
                        # Log in VS5 style with detailed statistics
                        if new_count > 0:
                            logger.info(f"[SEARCH] '{search['name']}': {total_items} total, {new_count} new, {duplicate_count} duplicates")
                            get_db().add_log_entry('INFO', 
                                           f"Search completed: {search['name']}", 
                                           'core', 
                                           f"Query ID {search['id']}: {total_items} total items, {new_count} new items, {duplicate_count} duplicates")
                        else:
                            logger.info(f"[SEARCH] '{search['name']}': {total_items} items found, all duplicates")
                            get_db().add_log_entry('INFO', 
                                           f"Search completed (all duplicates): {search['name']}", 
                                           'core', 
                                           f"Query ID {search['id']}: {total_items} items found, but all were duplicates")
                        
                        # Send telegram notifications for new items
                        for item in new_items:
                            try:
                                from simple_telegram_worker import send_notification_for_item
                                send_notification_for_item(item)
                                logger.debug(f"Telegram notification sent for item {item['kufar_id']}")
                            except Exception as e:
                                logger.error(f"Failed to send telegram notification: {e}")
                                get_db().add_log_entry('ERROR', f"Failed to send telegram notification: {str(e)}", 'core', f"Notification error for item {item['kufar_id']}")
                    else:
                        logger.info(f"[SEARCH] '{search['name']}': No items found")
                        get_db().add_log_entry('INFO', 
                                       f"Search completed (empty): {search['name']}", 
                                       'core', 
                                       f"Query ID {search['id']}: No items found on Kufar")
                    
                    results['successful_searches'] += 1
                    
                    # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð²Ñ€ÐµÐ¼Ñ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ³Ð¾ ÑÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
                    get_db().update_search_scan_time(search['id'])
                    logger.info(f"ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾ Ð²Ñ€ÐµÐ¼Ñ ÑÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ° '{search['name']}'")
                    
                    # Add delay between searches
                    time.sleep(random.uniform(2, 5))
                    
                except Exception as e:
                    logger.error(f"Failed to process search '{search['name']}': {e}")
                    results['failed_searches'] += 1
                    results['errors'].append({
                        'search_id': search['id'],
                        'search_name': search['name'],
                        'error': str(e)
                    })
                    
                    # Log error to database
                    if isinstance(e, KufarAPIException) and e.status_code:
                        get_db().log_error(e.status_code, str(e), search['id'])
                        
                        # Check if should trigger redeploy
                        if e.status_code in ERROR_CODES_FOR_REDEPLOY:
                            self.error_count += 1
            
            # ÐŸÐ¾Ð´Ñ€Ð¾Ð±Ð½Ð°Ñ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ
            logger.info(f"ðŸ Ð¦Ð¸ÐºÐ» ÑÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½:")
            logger.info(f"   â€¢ Ð’ÑÐµÐ³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ¾Ð²: {results['total_searches']}")
            logger.info(f"   â€¢ Ð“Ð¾Ñ‚Ð¾Ð²Ñ‹Ñ… Ðº ÑÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸ÑŽ: {results['ready_for_scan']}")
            logger.info(f"   â€¢ Ð£ÑÐ¿ÐµÑˆÐ½Ñ‹Ñ…: {results['successful_searches']}")
            logger.info(f"   â€¢ ÐÐµÑƒÐ´Ð°Ñ‡Ð½Ñ‹Ñ…: {results['failed_searches']}")
            logger.info(f"   â€¢ ÐŸÑ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½Ð½Ñ‹Ñ…: {results['skipped_searches']}")
            logger.info(f"   â€¢ ÐÐ¾Ð²Ñ‹Ñ… Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ð¹: {results['new_items']}")
            
            if results['ready_for_scan'] > 0:
                get_db().add_log_entry('INFO', 
                               f"Ð¦Ð¸ÐºÐ» Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½: {results['successful_searches']}/{results['ready_for_scan']} ÑƒÑÐ¿ÐµÑˆÐ½Ñ‹Ñ…, {results['new_items']} Ð½Ð¾Ð²Ñ‹Ñ…", 
                               'core', 
                               f"Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°: {results}")
            else:
                get_db().add_log_entry('INFO', 
                               f"ÐžÐ¶Ð¸Ð´Ð°Ð½Ð¸Ðµ: {results['skipped_searches']} Ð¿Ð¾Ð¸ÑÐºÐ¾Ð² Ð¶Ð´ÑƒÑ‚ Ð¸Ð½Ñ‚ÐµÑ€Ð²Ð°Ð»Ð° ({interval_seconds}Ñ)", 
                               'core', 
                               'Ð’ÑÐµ Ð¿Ð¾Ð¸ÑÐºÐ¸ Ð² Ñ€ÐµÐ¶Ð¸Ð¼Ðµ Ð¾Ð¶Ð¸Ð´Ð°Ð½Ð¸Ñ')
            
            return results
            
        except Exception as e:
            logger.error(f"Error in search_all_queries: {e}")
            results['errors'].append({'error': str(e)})
            return results
    
    def search_query(self, search: Dict[str, Any]) -> List[Any]:
        """Search for items using a single query"""
        try:
            if not self.kufar_client:
                self._init_kufar_client()
            
            # Search using Kufar API
            max_items_config = get_max_items_per_search()
            logger.info(f"ðŸ”§ Using MAX_ITEMS_PER_SEARCH setting: {max_items_config}")
            
            items = self.kufar_client.items.search(
                query_url=search['url'],
                max_items=max_items_config
            )
            
            logger.info(f"ðŸ“¦ Search returned {len(items)} items from Kufar for query '{search['name']}'")
            logger.info(f"ðŸŽ¯ Max items requested: {max_items_config}, actual items received: {len(items)}")
            
            # Update API request counter
            try:
                import metrics_storage
                total_requests = metrics_storage.metrics_storage.increment_api_requests()
                logger.info(f"ðŸ“Š API request #{total_requests} completed for search query '{search['name']}'")
                
                # Also increment in shared_state for compatibility
                try:
                    import shared_state
                    shared_state.increment_api_requests()
                except:
                    pass
            except Exception as e:
                logger.error(f"Failed to increment API counter: {e}")
                # Log this as it affects metrics accuracy
                get_db().add_log_entry('ERROR', f'Failed to increment API counter: {e}', 'core', 'Metrics tracking error')
            
            return items
            
        except KufarAPIException as e:
            logger.error(f"Kufar API error for search {search['id']}: {e}")
            
            # Still increment API counter for failed requests
            try:
                import metrics_storage
                total_requests = metrics_storage.metrics_storage.increment_api_requests()
                logger.info(f"ðŸ“Š API request #{total_requests} failed with error {e.status_code} for '{search['name']}'")
                
                # Log the failed API request
                get_db().add_log_entry('ERROR', f'Kufar API error {e.status_code}: {e}', 'core', f'Failed API request for search ID {search["id"]}')
                
                # Also increment in shared_state for compatibility
                try:
                    import shared_state
                    shared_state.increment_api_requests()
                except:
                    pass
            except Exception as counter_error:
                logger.error(f"Failed to increment API counter for error: {counter_error}")
            
            # Handle specific error codes
            if e.status_code in [403, 429]:
                self._change_proxy_if_needed()
            
            raise
            
        except Exception as e:
            logger.error(f"Unexpected error in search_query: {e}")
            
            # Still increment API counter for unexpected errors
            try:
                import metrics_storage
                total_requests = metrics_storage.metrics_storage.increment_api_requests()
                logger.info(f"ðŸ“Š API request #{total_requests} failed with unexpected error for '{search['name']}'")
                
                # Log the unexpected error
                get_db().add_log_entry('ERROR', f'Unexpected search error: {e}', 'core', f'Unexpected error for search ID {search["id"]}')
                
                # Also increment in shared_state for compatibility
                try:
                    import shared_state
                    shared_state.increment_api_requests()
                except:
                    pass
            except Exception as counter_error:
                logger.error(f"Failed to increment API counter for unexpected error: {counter_error}")
            
            raise
    
    def _process_new_items(self, items: List[Any], search: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Process and save new items to database"""
        new_items = []
        
        for item in items:
            try:
                # Try to add item to database
                from configuration_values import get_telegram_chat_id
                
                # Extract size from item if available
                size_info = getattr(item, 'size', '')
                logger.info(f"ðŸ” SIZE DEBUG: Item '{item.title}' has size attribute: '{size_info}'")
                
                # Prepare raw_data with size information
                raw_data = item.raw_data.copy() if item.raw_data else {}
                if size_info:
                    raw_data['size'] = size_info
                    logger.info(f"ðŸ” SIZE DEBUG: Added size '{size_info}' to raw_data for '{item.title}'")
                
                item_data = {
                    'kufar_id': item.id,
                    'title': item.title,
                    'price': item.price,
                    'currency': item.currency,
                    'description': item.description,
                    'images': item.images,
                    'location': item.location,
                    'seller_name': item.seller_name,
                    'seller_phone': item.seller_phone,
                    'url': item.url,
                    'raw_data': raw_data,
                    'search_name': search.get('name', 'Unknown'),
                    'thread_id': search.get('telegram_thread_id')
                }
                
                item_id = get_db().add_item(item_data, search['id'])
                
                if item_id:  # New item was added
                    new_items.append({
                        'id': item_id,
                        'kufar_id': item.id,
                        'title': item.title,
                        'price': item.price,
                        'currency': item.currency,
                        'url': item.url,
                        'search_name': search['name'],
                        'thread_id': search.get('telegram_thread_id'),
                        'images': item.images,
                        'location': item.location,
                        'description': item.description,
                        'raw_data': raw_data  # Include raw_data with size
                    })
                
            except Exception as e:
                logger.error(f"Failed to process item {item.id}: {e}")
                continue
        
        return new_items
    
    def validate_search_url(self, url: str) -> Dict[str, Any]:
        """Validate and parse Kufar search URL"""
        try:
            parsed_url = urlparse(url)
            
            # Check if it's a valid Kufar URL
            if 'kufar.by' not in parsed_url.netloc:
                return {'valid': False, 'error': 'Not a Kufar.by URL'}
            
            # Parse query parameters
            query_params = parse_qs(parsed_url.query)
            
            # Extract useful information
            info = {
                'valid': True,
                'url': url,
                'query': query_params.get('q', [''])[0],
                'category': self._get_category_name(query_params.get('cat', [''])[0]),
                'region': self._get_region_name(query_params.get('rgn', [''])[0]),
                'price_from': query_params.get('prif', [''])[0],
                'price_to': query_params.get('prit', [''])[0],
                'parameters': query_params
            }
            
            return info
            
        except Exception as e:
            logger.error(f"Error validating URL {url}: {e}")
            return {'valid': False, 'error': str(e)}
    
    def _get_category_name(self, category_id: str) -> str:
        """Get category name by ID"""
        # This would need to be populated with actual Kufar category mappings
        category_mapping = {
            '1000': 'Ð¢Ñ€Ð°Ð½ÑÐ¿Ð¾Ñ€Ñ‚',
            '2000': 'ÐÐµÐ´Ð²Ð¸Ð¶Ð¸Ð¼Ð¾ÑÑ‚ÑŒ',
            '3000': 'Ð Ð°Ð±Ð¾Ñ‚Ð°',
            '4000': 'Ð£ÑÐ»ÑƒÐ³Ð¸',
            '5000': 'Ð›Ð¸Ñ‡Ð½Ñ‹Ðµ Ð²ÐµÑ‰Ð¸',
            '6000': 'Ð”Ð»Ñ Ð´Ð¾Ð¼Ð° Ð¸ Ð´Ð°Ñ‡Ð¸',
            '7000': 'Ð‘Ñ‹Ñ‚Ð¾Ð²Ð°Ñ ÑÐ»ÐµÐºÑ‚Ñ€Ð¾Ð½Ð¸ÐºÐ°',
            '8000': 'Ð¥Ð¾Ð±Ð±Ð¸ Ð¸ Ð¾Ñ‚Ð´Ñ‹Ñ…',
            '9000': 'Ð–Ð¸Ð²Ð¾Ñ‚Ð½Ñ‹Ðµ Ð¸ Ñ€Ð°ÑÑ‚ÐµÐ½Ð¸Ñ',
            '10000': 'Ð”Ð»Ñ Ð±Ð¸Ð·Ð½ÐµÑÐ°'
        }
        
        return category_mapping.get(category_id, category_id)
    
    def _get_region_name(self, region_id: str) -> str:
        """Get region name by ID"""
        # This would need to be populated with actual Kufar region mappings
        region_mapping = {
            '1': 'ÐœÐ¸Ð½ÑÐº',
            '2': 'ÐœÐ¸Ð½ÑÐºÐ°Ñ Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
            '3': 'Ð‘Ñ€ÐµÑÑ‚ÑÐºÐ°Ñ Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
            '4': 'Ð’Ð¸Ñ‚ÐµÐ±ÑÐºÐ°Ñ Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
            '5': 'Ð“Ð¾Ð¼ÐµÐ»ÑŒÑÐºÐ°Ñ Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
            '6': 'Ð“Ñ€Ð¾Ð´Ð½ÐµÐ½ÑÐºÐ°Ñ Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
            '7': 'ÐœÐ¾Ð³Ð¸Ð»Ñ‘Ð²ÑÐºÐ°Ñ Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ'
        }
        
        return region_mapping.get(region_id, region_id)
    
    def get_searcher_status(self) -> Dict[str, Any]:
        """Get current searcher status"""
        return {
            'client_initialized': self.kufar_client is not None,
            'error_count': self.error_count,
            'proxy_info': self.kufar_client.get_session_info() if self.kufar_client else None,
            'last_proxy_change': self.last_proxy_change
        }
    
    def reset_error_count(self):
        """Reset error count (useful after successful redeploy)"""
        self.error_count = 0
        logger.info("Error count reset")

# Global searcher instance
searcher = KufarSearcher()
